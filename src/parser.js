// This file was generated by lezer-generator. You probably shouldn't edit it.
const {Parser} = require("lezer")
exports.parser = Parser.deserialize({
  version: 12,
  states: "$UOVQPOOOhQPO'#ClOpQPO'#C^QOQO'#Cl'#ClOuQPO'#ClQOQPOOO}QPO,58yO!YQPO'#C_OOQO,58x,58xO!_QPO'#CgOOQO'#Cg'#CgOuQPO'#CgO!mQPO,59WOOQO1G.e1G.eO!rQPO1G.eO!zQPO,59ROOQO1G.r1G.rOuQPO'#C`O#PQPO7+$POOQO7+$P7+$POOQO1G.m1G.mOOQO,58z,58zOOQO-E6^-E6^OOQO<<Gk<<Gk",
  stateData: "#X~OVOS~OWQOXPO[SOaRObRO~OYUOT`X~OXVO~OXXO[ZO~OXXO[ZO_]O~OYUO~OYUO]ZX^ZX_ZX~O]`O~O^aO_cO~O]dO~O^aO_gO~O",
  goto: "!TaPPbeqPPPPPPwPPPPbRTOQROQWQXYSUZaQb^RfbQ[SQ^UQ_ZRea",
  nodeNames: "âš  Grammar Top DefExpression",
  maxTerm: 18,
  skippedNodes: [0],
  repeatNodeCount: 1,
  tokenData: "$W~R_XY!QYZ!Q]^!Qpq!Qrs!Vwx!exy!syz!x!b!c!}!c!}#f#R#S#f#T#o#f#o#p#w#p#q#|#q#r$R~!VOV~~!YQrs!`!O!P!V~!eOa~~!hQwx!n!O!P!e~!sOb~~!xO[~~!}O]~~#QP#h#i#T~#WP#c#d#Z~#^P#d#e#a~#fOW~~#kSX~!Q![#f!c!}#f#R#S#f#T#o#f~#|OY~~$RO^~~$WO_~",
  tokenizers: [0],
  topRules: {"Grammar":[0,1]},
  tokenPrec: 0
})
